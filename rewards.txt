Naive: action = self.next_waypoint

sample of rewards: [18.0, 19.0, 16.0, 13.0, 21.0, 16.0, 21.0, 24.0, 12.0, 16.0, 17.0, 15.0, 18.0, 20.0, 18.0, 12.0, 15.0, 22.0, 12.0, 19.0, 28.0, 20.0, 15.0, 15.0, 13.0, 16.0, 20.0, 15.0, 18.0, 14.0, 11.0, 6.0, 14.0, 9.0, 18.0, 11.0, 10.0, 13.0, 13.0, 26.0, 15.0, 19.0, 12.0, 14.0, 22.0, 14.0, 11.0, 16.0, 16.0, 9.0, 22.0, 14.0, 6.0, 18.0, 18.0, 21.0, 15.0, 18.0, 20.0, 16.0, 18.0, 14.0, 12.0, 16.0, 15.0, 9.0, 9.0, 17.0, 11.0, 18.0, 17.0, 12.0, 15.0, 15.0, 9.0, 16.0, 9.0, 11.0, 16.0, 15.0, 18.0, 17.0, 22.0, 12.0, 17.0, 10.0, 9.0, 19.0, 15.0, 18.0, 13.0, 22.0, 18.0, 23.0, 20.0, 15.0, 14.0, 10.0, 20.0, 15.0]
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
6.00   13.00   15.50   15.66   18.00   28.00
Roughly normally distributed around the mean.
